<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<title>Nilorea Library: include/nilorea/n_neural_networks.h Source File</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javaScript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css"/>
</head>
<body onload='searchBox.OnSelectItem(0);'>
<!-- Generated by Doxygen 1.7.2 -->
<script type="text/javascript"><!--
var searchBox = new SearchBox("searchBox", "search",false,'Search');
--></script>
<div class="navigation" id="top">
  <div class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="annotated.html"><span>Data&#160;Structures</span></a></li>
      <li class="current"><a href="files.html"><span>Files</span></a></li>
      <li id="searchli">
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
  <div class="tabs2">
    <ul class="tablist">
      <li><a href="files.html"><span>File&#160;List</span></a></li>
      <li><a href="globals.html"><span>Globals</span></a></li>
    </ul>
  </div>
<div class="header">
  <div class="headertitle">
<h1>include/nilorea/n_neural_networks.h</h1>  </div>
</div>
<div class="contents">
<a href="n__neural__networks_8h.html">Go to the documentation of this file.</a><div class="fragment"><pre class="fragment"><a name="l00001"></a>00001 <span class="comment">/**\file n_neural_networks.h</span>
<a name="l00002"></a>00002 <span class="comment"> *  Neural network declarations</span>
<a name="l00003"></a>00003 <span class="comment"> *\author Castagnier Mickael</span>
<a name="l00004"></a>00004 <span class="comment"> *\version 1.0</span>
<a name="l00005"></a>00005 <span class="comment"> *\date 26/07/2015</span>
<a name="l00006"></a>00006 <span class="comment"> */</span>
<a name="l00007"></a>00007 <span class="preprocessor">#ifndef N_NEURAL_NETWORKS</span>
<a name="l00008"></a>00008 <span class="preprocessor"></span>
<a name="l00009"></a>00009 <span class="preprocessor">#define N_NEURAL_NETWORKS</span>
<a name="l00010"></a>00010 <span class="preprocessor"></span>
<a name="l00011"></a>00011 <span class="preprocessor">#ifdef __cplusplus</span>
<a name="l00012"></a>00012 <span class="preprocessor"></span><span class="keyword">extern</span> <span class="stringliteral">&quot;C&quot;</span>
<a name="l00013"></a>00013 {
<a name="l00014"></a>00014 <span class="preprocessor">#endif</span>
<a name="l00015"></a>00015 <span class="preprocessor"></span>
<a name="l00016"></a>00016 <span class="preprocessor">#include &quot;<a class="code" href="n__list_8h.html">n_list.h</a>&quot;</span>
<a name="l00017"></a>00017 
<a name="l00018"></a>00018 
<a name="l00019"></a>00019 <span class="preprocessor">#ifndef MIN</span>
<a name="l00020"></a>00020 <span class="preprocessor"></span><span class="preprocessor">                #define MIN(x,y)      ((x)&lt;(y) ? (x) : (y))</span>
<a name="l00021"></a>00021 <span class="preprocessor"></span><span class="preprocessor">#endif</span>
<a name="l00022"></a>00022 <span class="preprocessor"></span>
<a name="l00023"></a>00023 <span class="preprocessor">#ifndef MAX</span>
<a name="l00024"></a>00024 <span class="preprocessor"></span><span class="preprocessor">                #define MAX(x,y)      ((x)&gt;(y) ? (x) : (y))</span>
<a name="l00025"></a>00025 <span class="preprocessor"></span><span class="preprocessor">#endif</span>
<a name="l00026"></a>00026 <span class="preprocessor"></span>
<a name="l00027"></a>00027 <span class="preprocessor">#ifndef sqr</span>
<a name="l00028"></a>00028 <span class="preprocessor"></span><span class="preprocessor">                #define sqr(x)        ((x)*(x))</span>
<a name="l00029"></a>00029 <span class="preprocessor"></span><span class="preprocessor">#endif</span>
<a name="l00030"></a>00030 <span class="preprocessor"></span>
<a name="l00031"></a>00031 
<a name="l00032"></a>00032 <span class="comment">/* randome value between int a and int b */</span>
<a name="l00033"></a>00033 <span class="keywordtype">int</span> rand_between_ints( <span class="keywordtype">int</span> a , <span class="keywordtype">int</span> b );    
<a name="l00034"></a>00034 
<a name="l00035"></a>00035 <span class="comment">/* randome value between double a and double b */</span>
<a name="l00036"></a>00036 <span class="keywordtype">double</span> rand_between_doubles( <span class="keywordtype">double</span> a , <span class="keywordtype">double</span> b );
<a name="l00037"></a>00037 
<a name="l00038"></a>00038 <span class="keywordtype">double</span> rand_equal_double( <span class="keywordtype">double</span> a , <span class="keywordtype">double</span> b );
<a name="l00039"></a>00039 
<a name="l00040"></a>00040 <span class="keywordtype">int</span> rand_equal_int( <span class="keywordtype">int</span> a , <span class="keywordtype">int</span> b );
<a name="l00041"></a>00041 
<a name="l00042"></a>00042 <span class="comment"></span>
<a name="l00043"></a>00043 <span class="comment">/*! basic neural network activation function: sigmoid */</span>
<a name="l00044"></a>00044 <span class="keywordtype">double</span> n_neural_sigmoid ( <span class="keywordtype">double</span> x ) ;
<a name="l00045"></a>00045 
<a name="l00046"></a>00046 <span class="comment"></span>
<a name="l00047"></a>00047 <span class="comment">/*! Structure of a perceptron */</span>
<a name="l00048"></a><a class="code" href="struct_n___p_e_r_c_e_p_t_r_o_n.html">00048</a> <span class="keyword">typedef</span> <span class="keyword">struct </span><a class="code" href="struct_n___p_e_r_c_e_p_t_r_o_n.html">N_PERCEPTRON</a>
<a name="l00049"></a>00049 {<span class="comment"></span>
<a name="l00050"></a>00050 <span class="comment">                /*! list of N_PERCEPTRON inputs for normal layer computing */</span>
<a name="l00051"></a><a class="code" href="struct_n___p_e_r_c_e_p_t_r_o_n.html#a8ee3b2fdfc6f657e90fbbfa185cdb844">00051</a>                 <a class="code" href="struct_l_i_s_t.html">LIST</a> *<a class="code" href="struct_n___p_e_r_c_e_p_t_r_o_n.html#a03745329c6c93cbf5b5da69648166d75">intput_links</a>,<span class="comment"></span>
<a name="l00052"></a>00052 <span class="comment">                /*! list of N_PERCEPTRON inputs for signal generated layer computing */</span>
<a name="l00053"></a>00053                      *<a class="code" href="struct_n___p_e_r_c_e_p_t_r_o_n.html#a8ee3b2fdfc6f657e90fbbfa185cdb844">outputs_links</a> ;
<a name="l00054"></a>00054                                 <span class="comment"></span>
<a name="l00055"></a>00055 <span class="comment">                                /*! perceptron&#39;s bias */</span>
<a name="l00056"></a><a class="code" href="struct_n___p_e_r_c_e_p_t_r_o_n.html#a39244e34268e768eefe8843add1d94ca">00056</a>                                 <span class="keywordtype">double</span> <a class="code" href="struct_n___p_e_r_c_e_p_t_r_o_n.html#a0407e5524ad650edc073537bc45471a2">bias_weights</a> ,<span class="comment"></span>
<a name="l00057"></a>00057 <span class="comment">                                /*! perceptron activation value */</span> 
<a name="l00058"></a>00058                                 <a class="code" href="struct_n___p_e_r_c_e_p_t_r_o_n.html#a39244e34268e768eefe8843add1d94ca">treshold</a> ,<span class="comment"></span>
<a name="l00059"></a>00059 <span class="comment">                                /*! computed output */</span>
<a name="l00060"></a>00060                                 <a class="code" href="struct_n___p_e_r_c_e_p_t_r_o_n.html#a47943f73244ca117fa9bf9234cbed317">output</a> ,<span class="comment"></span>
<a name="l00061"></a>00061 <span class="comment">                                /*!  learning rate */</span>
<a name="l00062"></a>00062                                 <a class="code" href="struct_n___p_e_r_c_e_p_t_r_o_n.html#a9ea1a8f5ef3e6d20b362385e1288c72c">delta</a> ,<span class="comment"></span>
<a name="l00063"></a>00063 <span class="comment">                                /*! momentum */</span>
<a name="l00064"></a>00064                                 <a class="code" href="struct_n___p_e_r_c_e_p_t_r_o_n.html#a20f4c4490bc8ecbdd1ffcb79acce6035">alpha</a> ,<span class="comment"></span>
<a name="l00065"></a>00065 <span class="comment">                                /*! perceptron error */</span>
<a name="l00066"></a>00066                                 <a class="code" href="struct_n___p_e_r_c_e_p_t_r_o_n.html#a1ec236df65b93a338aecdf3fa64790f9">error</a>  ;
<a name="l00067"></a>00067                                    <span class="comment"></span>
<a name="l00068"></a>00068 <span class="comment">                /*! pointer to activation function */</span>              
<a name="l00069"></a><a class="code" href="struct_n___p_e_r_c_e_p_t_r_o_n.html#a0d4649bec57904dc1818315ae226aef9">00069</a>                 double(*<a class="code" href="struct_n___p_e_r_c_e_p_t_r_o_n.html#a0d4649bec57904dc1818315ae226aef9">a_func</a>)(<span class="keywordtype">double</span> val );
<a name="l00070"></a>00070                 
<a name="l00071"></a>00071 }<a class="code" href="n__neural__networks_8h.html#a72defe5c3f61d4e4604d8f7b5f83acd4">N_PERCEPTRON</a> ;
<a name="l00072"></a>00072 
<a name="l00073"></a>00073 <span class="comment"></span>
<a name="l00074"></a>00074 <span class="comment">/*! One layer of percetron, size_x by size_y, some can be NULL ones */</span>
<a name="l00075"></a>00075 <span class="keyword">typedef</span> <span class="keyword">struct </span><a class="code" href="struct_n___n_e_u_r_a_l___l_a_y_e_r.html">N_NEURAL_LAYER</a>
<a name="l00076"></a>00076 {<span class="comment"></span>
<a name="l00077"></a>00077 <span class="comment">                /*! Width of the layer */</span>
<a name="l00078"></a>00078                 <span class="keywordtype">int</span> <a class="code" href="struct_n___n_e_u_r_a_l___l_a_y_e_r.html#aec1dee03c858e0d3910c40c209890ab0">size_x</a> ,<span class="comment"></span>
<a name="l00079"></a>00079 <span class="comment">                /*!             Height of the layer */</span>
<a name="l00080"></a>00080                 <a class="code" href="struct_n___n_e_u_r_a_l___l_a_y_e_r.html#a22aeb82e7a98517be5f90c810e423af0">size_y</a> ;<span class="comment"></span>
<a name="l00081"></a>00081 <span class="comment">                /*! Dynamically allocated array of perceptron. Some can be NULL. */</span>
<a name="l00082"></a><a class="code" href="struct_n___n_e_u_r_a_l___l_a_y_e_r.html#a0b284dafc32f16cc6abe749fbe9be756">00082</a>                 <a class="code" href="struct_n___p_e_r_c_e_p_t_r_o_n.html">N_PERCEPTRON</a> **<a class="code" href="struct_n___n_e_u_r_a_l___l_a_y_e_r.html#a0b284dafc32f16cc6abe749fbe9be756">array</a> ;
<a name="l00083"></a>00083                 
<a name="l00084"></a>00084 } <a class="code" href="n__neural__networks_8h.html#a825600e10a0f7c67dfddf7293ac46176">N_NEURAL_LAYER</a> ;
<a name="l00085"></a>00085 
<a name="l00086"></a>00086 
<a name="l00087"></a>00087 <span class="comment"></span>
<a name="l00088"></a>00088 <span class="comment">/*! structure of a Neural Network, a group of perceptron */</span>
<a name="l00089"></a><a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html">00089</a> <span class="keyword">typedef</span> <span class="keyword">struct </span><a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html">NEURAL_NETWORK</a>
<a name="l00090"></a>00090 {<span class="comment"></span>
<a name="l00091"></a>00091 <span class="comment">                /*! name of the neural network */</span>
<a name="l00092"></a><a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#a5ac083a645d964373f022d03df4849c8">00092</a>                 <span class="keywordtype">char</span> *<a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#a5ac083a645d964373f022d03df4849c8">name</a> ;
<a name="l00093"></a>00093 <span class="comment"></span>
<a name="l00094"></a>00094 <span class="comment">                /*! list of list of N_NEURAL_LAYER */</span>
<a name="l00095"></a><a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#a61670dd6b8717917c7e07f005a4600c5">00095</a>                 <a class="code" href="struct_l_i_s_t.html">LIST</a> *<a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#a61670dd6b8717917c7e07f005a4600c5">network</a> ;
<a name="l00096"></a>00096                 <span class="comment"></span>
<a name="l00097"></a>00097 <span class="comment">                /*! pointer to input data */</span>
<a name="l00098"></a><a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#a627767a77a706a7dae5f105018a5606c">00098</a>                 <span class="keywordtype">double</span> **<a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#a627767a77a706a7dae5f105018a5606c">input</a> ;
<a name="l00099"></a>00099                 <span class="comment"></span>
<a name="l00100"></a>00100 <span class="comment">                /*! Width of input data, used to create first (input) layer */</span>
<a name="l00101"></a><a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#a1032bb83aecf2a1d1ddd627630f25877">00101</a>                 <span class="keywordtype">int</span> <a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#a33b9a821b4aee29fd7b6b853e9ff8335">input_x</a> ,<span class="comment"></span>
<a name="l00102"></a>00102 <span class="comment">                /*! Height of input data, used to create first (input) layer */</span>
<a name="l00103"></a>00103                                 <a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#a1fc83259374cb4a3cab5d06e7aa92d82">input_y</a> ,<span class="comment"></span>
<a name="l00104"></a>00104 <span class="comment">                /*! current number of layers */</span>
<a name="l00105"></a>00105                                 <a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#a2d1792668f5382f6c88329676ed9e2ee">nb_layers</a> ,<span class="comment"></span>
<a name="l00106"></a>00106 <span class="comment">                /*! Total number of active perceptrons */</span>
<a name="l00107"></a>00107                                 <a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#a1032bb83aecf2a1d1ddd627630f25877">nb_total</a> ;
<a name="l00108"></a>00108 <span class="comment"></span>
<a name="l00109"></a>00109 <span class="comment">                /*! computed output */</span>
<a name="l00110"></a><a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#a64593065c0adcda7054814b551808b55">00110</a>                 <span class="keywordtype">double</span> <a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#a47943f73244ca117fa9bf9234cbed317">output</a> ,<span class="comment"></span>
<a name="l00111"></a>00111 <span class="comment">                                                /*! momentum */</span>
<a name="l00112"></a>00112                                                 <a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#a20f4c4490bc8ecbdd1ffcb79acce6035">alpha</a> ,<span class="comment"></span>
<a name="l00113"></a>00113 <span class="comment">                                                /*! learning_rate */</span>
<a name="l00114"></a>00114                                                 <a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#a7b45f711c59e30b2aa59c4c954ad8ea1">learning_rate</a> ,<span class="comment"></span>
<a name="l00115"></a>00115 <span class="comment">                                                /*! activation_func gain */</span>
<a name="l00116"></a>00116                                                 <a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#ac7b389a0254ed87600f959a5f6d58ab0">gain</a> ,<span class="comment"></span>
<a name="l00117"></a>00117 <span class="comment">                                                /*! total of network errors */</span>
<a name="l00118"></a>00118                                                 <a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#a1ec236df65b93a338aecdf3fa64790f9">error</a> ,<span class="comment"></span>
<a name="l00119"></a>00119 <span class="comment">                                                /*! Ouput mid value (not sure) */</span>
<a name="l00120"></a>00120                                                 <a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#ad52c704ca84960065c1bfd5e5e15faab">Mean</a> ,<span class="comment"></span>
<a name="l00121"></a>00121 <span class="comment">                                                /*! Error level after training */</span>
<a name="l00122"></a>00122                                                 <a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#a3f437892e8adea84b5ec65843bfbcd06">TrainError</a> ,<span class="comment"></span>
<a name="l00123"></a>00123 <span class="comment">                                                /*! Mean Error level after training */</span>
<a name="l00124"></a>00124                                                 <a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#a64593065c0adcda7054814b551808b55">TrainErrorPredictingMean</a> ,<span class="comment"></span>
<a name="l00125"></a>00125 <span class="comment">                                                /*! Error level after testing */</span>                                                
<a name="l00126"></a>00126                                                 <a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#afd471c2776f6ca483ec35a35039814c6">TestError</a> ,<span class="comment"></span>
<a name="l00127"></a>00127 <span class="comment">                                                /*! Mean Error level after testing */</span>
<a name="l00128"></a>00128                                                 <a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html#afaa5ea488b30af21bc9ce5dfdc50c5cd">TestErrorPredictingMean</a>;                        
<a name="l00129"></a>00129 }<a class="code" href="n__neural__networks_8h.html#ad1718bc59af5056cebf47ceba40ff445">NEURAL_NETWORK</a> ;
<a name="l00130"></a>00130 
<a name="l00131"></a>00131 <a class="code" href="struct_n___p_e_r_c_e_p_t_r_o_n.html">N_PERCEPTRON</a> *new_perceptron( <span class="keywordtype">double</span> bias_weights , <span class="keywordtype">double</span> treshold , <span class="keywordtype">double</span> output , <span class="keywordtype">double</span> delta , <span class="keywordtype">double</span> momentum , <span class="keywordtype">double</span>(*a_func)(<span class="keywordtype">double</span> val ) );
<a name="l00132"></a>00132                                 
<a name="l00133"></a>00133 <span class="comment">/* at first call it will also create and set to NULL neural_net -&gt; ptr , input_x , input_y  */</span>
<a name="l00134"></a>00134 <a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html">NEURAL_NETWORK</a> *new_neural_network( <span class="keywordtype">int</span> input_x , <span class="keywordtype">int</span> input_y );
<a name="l00135"></a>00135 
<a name="l00136"></a>00136 <span class="comment">/* add a new layer */</span>
<a name="l00137"></a>00137 <span class="keywordtype">int</span> add_neural_net_layer( <a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html">NEURAL_NETWORK</a> *neural_net , <span class="keywordtype">int</span> x , <span class="keywordtype">int</span> y );
<a name="l00138"></a>00138 <span class="keywordtype">int</span> set_perceptron( <a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html">NEURAL_NETWORK</a> *neural_net , <span class="keywordtype">int</span> layer , <span class="keywordtype">int</span> x , <span class="keywordtype">int</span> y , <span class="keywordtype">int</span> a_func , <span class="keywordtype">double</span> bias , <span class="keywordtype">double</span> treshold , <span class="keywordtype">double</span> output );
<a name="l00139"></a>00139 <span class="keywordtype">int</span> unset_perceptron( <a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html">NEURAL_NETWORK</a> *neural_net , <span class="keywordtype">int</span> layer , <span class="keywordtype">int</span> x , <span class="keywordtype">int</span> y );
<a name="l00140"></a>00140 <span class="keywordtype">int</span> add_perceptron_input( <a class="code" href="struct_n___p_e_r_c_e_p_t_r_o_n.html">N_PERCEPTRON</a> *n_perceptron , <a class="code" href="struct_n___p_e_r_c_e_p_t_r_o_n.html">N_PERCEPTRON</a> *input );
<a name="l00141"></a>00141 <span class="keywordtype">int</span> neural_net_set_input( NEURAK_NETWORK *neural_net , <span class="keywordtype">double</span> **input );
<a name="l00142"></a>00142 <span class="keywordtype">int</span> neural_net_compute( <a class="code" href="struct_n_e_u_r_a_l___n_e_t_w_o_r_k.html">NEURAL_NETWORK</a> *neural_net , <span class="keywordtype">double</span> *output );
<a name="l00143"></a>00143 
<a name="l00144"></a>00144 
<a name="l00145"></a>00145 
<a name="l00146"></a>00146 <span class="preprocessor">#ifdef __cplusplus</span>
<a name="l00147"></a>00147 <span class="preprocessor"></span>}
<a name="l00148"></a>00148 <span class="preprocessor">#endif</span>
<a name="l00149"></a>00149 <span class="preprocessor"></span>
<a name="l00150"></a>00150 <span class="preprocessor">#endif</span>
<a name="l00151"></a>00151 <span class="preprocessor"></span>
<a name="l00152"></a>00152 <span class="comment">/******************************************************************************</span>
<a name="l00153"></a>00153 <span class="comment">                     P R O P A G A T I N G   S I G N A L S</span>
<a name="l00154"></a>00154 <span class="comment"> ******************************************************************************/</span>
<a name="l00155"></a>00155 
<a name="l00156"></a>00156 
<a name="l00157"></a>00157 <span class="keywordtype">void</span> PropagateLayer(NET* Net, LAYER* Lower, LAYER* Upper)
<a name="l00158"></a>00158 {
<a name="l00159"></a>00159   INT  i,j;
<a name="l00160"></a>00160   REAL Sum;
<a name="l00161"></a>00161 
<a name="l00162"></a>00162   <span class="keywordflow">for</span> (i=1; i&lt;=Upper-&gt;Units; i++) {
<a name="l00163"></a>00163     Sum = 0;
<a name="l00164"></a>00164     <span class="keywordflow">for</span> (j=0; j&lt;=Lower-&gt;Units; j++) {
<a name="l00165"></a>00165       Sum += Upper-&gt;Weight[i][j] * Lower-&gt;Output[j];
<a name="l00166"></a>00166     }
<a name="l00167"></a>00167     Upper-&gt;Output[i] = 1 / (1 + exp(-Net-&gt;Gain * Sum));
<a name="l00168"></a>00168   }
<a name="l00169"></a>00169 }
<a name="l00170"></a>00170 
<a name="l00171"></a>00171 
<a name="l00172"></a>00172 <span class="keywordtype">void</span> neural_net_compute(NET* Net)
<a name="l00173"></a>00173 {
<a name="l00174"></a>00174   INT l;
<a name="l00175"></a>00175    
<a name="l00176"></a>00176   <span class="keywordflow">for</span> (l=0; l&lt;NUM_LAYERS-1; l++) {
<a name="l00177"></a>00177     PropagateLayer(Net, Net-&gt;Layer[l], Net-&gt;Layer[l+1]);
<a name="l00178"></a>00178   }
<a name="l00179"></a>00179 }
<a name="l00180"></a>00180 
<a name="l00181"></a>00181 <span class="comment">/******************************************************************************</span>
<a name="l00182"></a>00182 <span class="comment">                  B A C K P R O P A G A T I N G   E R R O R S</span>
<a name="l00183"></a>00183 <span class="comment"> ******************************************************************************/</span>
<a name="l00184"></a>00184 
<a name="l00185"></a>00185 
<a name="l00186"></a>00186 <span class="keywordtype">void</span> ComputeOutputError(NET* Net, REAL* Target)
<a name="l00187"></a>00187 {
<a name="l00188"></a>00188   INT  i;
<a name="l00189"></a>00189   REAL Out, Err;
<a name="l00190"></a>00190    
<a name="l00191"></a>00191   Net-&gt;Error = 0;
<a name="l00192"></a>00192   <span class="keywordflow">for</span> (i=1; i&lt;=Net-&gt;OutputLayer-&gt;Units; i++) {
<a name="l00193"></a>00193     Out = Net-&gt;OutputLayer-&gt;Output[i];
<a name="l00194"></a>00194     Err = Target[i-1]-Out;
<a name="l00195"></a>00195     Net-&gt;OutputLayer-&gt;Error[i] = Net-&gt;Gain * Out * (1-Out) * Err;
<a name="l00196"></a>00196     Net-&gt;Error += 0.5 * sqr(Err);
<a name="l00197"></a>00197   }
<a name="l00198"></a>00198 }
<a name="l00199"></a>00199 
<a name="l00200"></a>00200 
<a name="l00201"></a>00201 <span class="keywordtype">void</span> BackpropagateLayer(NET* Net, LAYER* Upper, LAYER* Lower)
<a name="l00202"></a>00202 {
<a name="l00203"></a>00203   INT  i,j;
<a name="l00204"></a>00204   REAL Out, Err;
<a name="l00205"></a>00205    
<a name="l00206"></a>00206   <span class="keywordflow">for</span> (i=1; i&lt;=Lower-&gt;Units; i++) {
<a name="l00207"></a>00207     Out = Lower-&gt;Output[i];
<a name="l00208"></a>00208     Err = 0;
<a name="l00209"></a>00209     <span class="keywordflow">for</span> (j=1; j&lt;=Upper-&gt;Units; j++) {
<a name="l00210"></a>00210       Err += Upper-&gt;Weight[j][i] * Upper-&gt;Error[j];
<a name="l00211"></a>00211     }
<a name="l00212"></a>00212     Lower-&gt;Error[i] = Net-&gt;Gain * Out * (1-Out) * Err;
<a name="l00213"></a>00213   }
<a name="l00214"></a>00214 }
<a name="l00215"></a>00215 
<a name="l00216"></a>00216 
<a name="l00217"></a>00217 <span class="keywordtype">void</span> BackpropagateNet(NET* Net)
<a name="l00218"></a>00218 {
<a name="l00219"></a>00219   INT l;
<a name="l00220"></a>00220    
<a name="l00221"></a>00221   <span class="keywordflow">for</span> (l=NUM_LAYERS-1; l&gt;1; l--) {
<a name="l00222"></a>00222     BackpropagateLayer(Net, Net-&gt;Layer[l], Net-&gt;Layer[l-1]);
<a name="l00223"></a>00223   }
<a name="l00224"></a>00224 }
<a name="l00225"></a>00225 
<a name="l00226"></a>00226 <span class="keywordtype">void</span> AdjustWeights(NET* Net)
<a name="l00227"></a>00227 {
<a name="l00228"></a>00228   INT  l,i,j;
<a name="l00229"></a>00229   REAL Out, Err, dWeight;
<a name="l00230"></a>00230    
<a name="l00231"></a>00231   <span class="keywordflow">for</span> (l=1; l&lt;NUM_LAYERS; l++) {
<a name="l00232"></a>00232     <span class="keywordflow">for</span> (i=1; i&lt;=Net-&gt;Layer[l]-&gt;Units; i++) {
<a name="l00233"></a>00233       <span class="keywordflow">for</span> (j=0; j&lt;=Net-&gt;Layer[l-1]-&gt;Units; j++) {
<a name="l00234"></a>00234         Out = Net-&gt;Layer[l-1]-&gt;Output[j];
<a name="l00235"></a>00235         Err = Net-&gt;Layer[l]-&gt;Error[i];
<a name="l00236"></a>00236         dWeight = Net-&gt;Layer[l]-&gt;dWeight[i][j];
<a name="l00237"></a>00237         Net-&gt;Layer[l]-&gt;Weight[i][j] += Net-&gt;Eta * Err * Out + Net-&gt;Alpha * dWeight;
<a name="l00238"></a>00238         Net-&gt;Layer[l]-&gt;dWeight[i][j] = Net-&gt;Eta * Err * Out;
<a name="l00239"></a>00239       }
<a name="l00240"></a>00240     }
<a name="l00241"></a>00241   }
<a name="l00242"></a>00242 }
<a name="l00243"></a>00243 
<a name="l00244"></a>00244 
<a name="l00245"></a>00245 <span class="comment">/******************************************************************************</span>
<a name="l00246"></a>00246 <span class="comment">                      S I M U L A T I N G   T H E   N E T</span>
<a name="l00247"></a>00247 <span class="comment"> ******************************************************************************/</span>
<a name="l00248"></a>00248 
<a name="l00249"></a>00249 
<a name="l00250"></a>00250 <span class="keywordtype">void</span> SimulateNet(NET* Net, REAL* Input, REAL* Output, REAL* Target, BOOL Training)
<a name="l00251"></a>00251 {
<a name="l00252"></a>00252   SetInput(Net, Input);
<a name="l00253"></a>00253   neural_net_compute(Net);
<a name="l00254"></a>00254   GetOutput(Net, Output);
<a name="l00255"></a>00255    
<a name="l00256"></a>00256   ComputeOutputError(Net, Target);
<a name="l00257"></a>00257   <span class="keywordflow">if</span> (Training) {
<a name="l00258"></a>00258     BackpropagateNet(Net);
<a name="l00259"></a>00259     AdjustWeights(Net);
<a name="l00260"></a>00260   }
<a name="l00261"></a>00261 }
<a name="l00262"></a>00262 
<a name="l00263"></a>00263 
<a name="l00264"></a>00264 <span class="keywordtype">void</span> TrainNet(NET* Net, INT Epochs)
<a name="l00265"></a>00265 {
<a name="l00266"></a>00266   INT  Year, n;
<a name="l00267"></a>00267   REAL Output[M];
<a name="l00268"></a>00268 
<a name="l00269"></a>00269   <span class="keywordflow">for</span> (n=0; n&lt;Epochs*TRAIN_YEARS; n++) {
<a name="l00270"></a>00270     Year = RandomEqualINT(TRAIN_LWB, TRAIN_UPB);
<a name="l00271"></a>00271     SimulateNet(Net, &amp;(Sunspots[Year-N]), Output, &amp;(Sunspots[Year]), <a class="code" href="group___c_o_m_m_o_n_s.html#gaa8cecfc5c5c054d2875c03e77b7be15d">TRUE</a>);
<a name="l00272"></a>00272   }
<a name="l00273"></a>00273 }
<a name="l00274"></a>00274 
<a name="l00275"></a>00275 
<a name="l00276"></a>00276 <span class="keywordtype">void</span> TestNet(NET* Net)
<a name="l00277"></a>00277 {
<a name="l00278"></a>00278   INT  Year;
<a name="l00279"></a>00279   REAL Output[M];
<a name="l00280"></a>00280 
<a name="l00281"></a>00281   TrainError = 0;
<a name="l00282"></a>00282   <span class="keywordflow">for</span> (Year=TRAIN_LWB; Year&lt;=TRAIN_UPB; Year++) {
<a name="l00283"></a>00283     SimulateNet(Net, &amp;(Sunspots[Year-N]), Output, &amp;(Sunspots[Year]), <a class="code" href="group___c_o_m_m_o_n_s.html#gaa93f0eb578d23995850d61f7d61c55c1">FALSE</a>);
<a name="l00284"></a>00284     TrainError += Net-&gt;Error;
<a name="l00285"></a>00285   }
<a name="l00286"></a>00286   TestError = 0;
<a name="l00287"></a>00287   <span class="keywordflow">for</span> (Year=TEST_LWB; Year&lt;=TEST_UPB; Year++) {
<a name="l00288"></a>00288     SimulateNet(Net, &amp;(Sunspots[Year-N]), Output, &amp;(Sunspots[Year]), <a class="code" href="group___c_o_m_m_o_n_s.html#gaa93f0eb578d23995850d61f7d61c55c1">FALSE</a>);
<a name="l00289"></a>00289     TestError += Net-&gt;Error;
<a name="l00290"></a>00290   }
<a name="l00291"></a>00291   fprintf(f, <span class="stringliteral">&quot;\nNMSE is %0.3f on Training Set and %0.3f on Test Set&quot;</span>,
<a name="l00292"></a>00292              TrainError / TrainErrorPredictingMean,
<a name="l00293"></a>00293              TestError / TestErrorPredictingMean);
<a name="l00294"></a>00294 }
<a name="l00295"></a>00295 
<a name="l00296"></a>00296 
<a name="l00297"></a>00297 <span class="keywordtype">void</span> EvaluateNet(NET* Net)
<a name="l00298"></a>00298 {
<a name="l00299"></a>00299   INT  Year;
<a name="l00300"></a>00300   REAL Output [M];
<a name="l00301"></a>00301   REAL Output_[M];
<a name="l00302"></a>00302 
<a name="l00303"></a>00303   fprintf(f, <span class="stringliteral">&quot;\n\n\n&quot;</span>);
<a name="l00304"></a>00304   fprintf(f, <span class="stringliteral">&quot;Year    Sunspots    Open-Loop Prediction    Closed-Loop Prediction\n&quot;</span>);
<a name="l00305"></a>00305   fprintf(f, <span class="stringliteral">&quot;\n&quot;</span>);
<a name="l00306"></a>00306   <span class="keywordflow">for</span> (Year=EVAL_LWB; Year&lt;=EVAL_UPB; Year++) {
<a name="l00307"></a>00307     SimulateNet(Net, &amp;(Sunspots [Year-N]), Output,  &amp;(Sunspots [Year]), <a class="code" href="group___c_o_m_m_o_n_s.html#gaa93f0eb578d23995850d61f7d61c55c1">FALSE</a>);
<a name="l00308"></a>00308     SimulateNet(Net, &amp;(Sunspots_[Year-N]), Output_, &amp;(Sunspots_[Year]), <a class="code" href="group___c_o_m_m_o_n_s.html#gaa93f0eb578d23995850d61f7d61c55c1">FALSE</a>);
<a name="l00309"></a>00309     Sunspots_[Year] = Output_[0];
<a name="l00310"></a>00310     fprintf(f, <span class="stringliteral">&quot;%d       %0.3f                   %0.3f                     %0.3f\n&quot;</span>,
<a name="l00311"></a>00311                FIRST_YEAR + Year,
<a name="l00312"></a>00312                Sunspots[Year],
<a name="l00313"></a>00313                Output [0],
<a name="l00314"></a>00314                Output_[0]);
<a name="l00315"></a>00315   }
<a name="l00316"></a>00316 }
<a name="l00317"></a>00317 
<a name="l00318"></a>00318 
<a name="l00319"></a>00319 <span class="comment">/******************************************************************************</span>
<a name="l00320"></a>00320 <span class="comment">                                    M A I N</span>
<a name="l00321"></a>00321 <span class="comment"> ******************************************************************************/</span>
<a name="l00322"></a>00322 
<a name="l00323"></a>00323 
<a name="l00324"></a>00324 <span class="keywordtype">void</span> main()
<a name="l00325"></a>00325 {
<a name="l00326"></a>00326   NET  Net;
<a name="l00327"></a>00327   BOOL Stop;
<a name="l00328"></a>00328   REAL MinTestError;
<a name="l00329"></a>00329 
<a name="l00330"></a>00330   InitializeRandoms();
<a name="l00331"></a>00331   GenerateNetwork(&amp;Net);
<a name="l00332"></a>00332   RandomWeights(&amp;Net);
<a name="l00333"></a>00333   InitializeApplication(&amp;Net);
<a name="l00334"></a>00334 
<a name="l00335"></a>00335   Stop = <a class="code" href="group___c_o_m_m_o_n_s.html#gaa93f0eb578d23995850d61f7d61c55c1">FALSE</a>;
<a name="l00336"></a>00336   MinTestError = DBL_MAX;
<a name="l00337"></a>00337   <span class="keywordflow">do</span> {
<a name="l00338"></a>00338     TrainNet(&amp;Net, 10);
<a name="l00339"></a>00339     TestNet(&amp;Net);
<a name="l00340"></a>00340     <span class="keywordflow">if</span> (TestError &lt; MinTestError) {
<a name="l00341"></a>00341       fprintf(f, <span class="stringliteral">&quot; - saving Weights ...&quot;</span>);
<a name="l00342"></a>00342       MinTestError = TestError;
<a name="l00343"></a>00343       SaveWeights(&amp;Net);
<a name="l00344"></a>00344     }
<a name="l00345"></a>00345     <span class="keywordflow">else</span> <span class="keywordflow">if</span> (TestError &gt; 1.2 * MinTestError) {
<a name="l00346"></a>00346       fprintf(f, <span class="stringliteral">&quot; - stopping Training and restoring Weights ...&quot;</span>);
<a name="l00347"></a>00347       Stop = <a class="code" href="group___c_o_m_m_o_n_s.html#gaa8cecfc5c5c054d2875c03e77b7be15d">TRUE</a>;
<a name="l00348"></a>00348       RestoreWeights(&amp;Net);
<a name="l00349"></a>00349     }
<a name="l00350"></a>00350   } <span class="keywordflow">while</span> (NOT Stop);
<a name="l00351"></a>00351 
<a name="l00352"></a>00352   TestNet(&amp;Net);
<a name="l00353"></a>00353   EvaluateNet(&amp;Net);
<a name="l00354"></a>00354    
<a name="l00355"></a>00355   FinalizeApplication(&amp;Net);
<a name="l00356"></a>00356 }
<a name="l00357"></a>00357 Simulator Output <span class="keywordflow">for</span> the Time-Series Forecasting Application
<a name="l00358"></a>00358 
<a name="l00359"></a>00359 NMSE is 0.879 on Training Set and 0.834 on Test Set - saving Weights ...
<a name="l00360"></a>00360 NMSE is 0.818 on Training Set and 0.783 on Test Set - saving Weights ...
<a name="l00361"></a>00361 NMSE is 0.749 on Training Set and 0.693 on Test Set - saving Weights ...
<a name="l00362"></a>00362 NMSE is 0.691 on Training Set and 0.614 on Test Set - saving Weights ...
<a name="l00363"></a>00363 NMSE is 0.622 on Training Set and 0.555 on Test Set - saving Weights ...
<a name="l00364"></a>00364 NMSE is 0.569 on Training Set and 0.491 on Test Set - saving Weights ...
<a name="l00365"></a>00365 NMSE is 0.533 on Training Set and 0.467 on Test Set - saving Weights ...
<a name="l00366"></a>00366 NMSE is 0.490 on Training Set and 0.416 on Test Set - saving Weights ...
<a name="l00367"></a>00367 NMSE is 0.470 on Training Set and 0.401 on Test Set - saving Weights ...
<a name="l00368"></a>00368 NMSE is 0.441 on Training Set and 0.361 on Test Set - saving Weights ...
<a name="l00369"></a>00369 .
<a name="l00370"></a>00370 .
<a name="l00371"></a>00371 .
<a name="l00372"></a>00372 NMSE is 0.142 on Training Set and 0.143 on Test Set
<a name="l00373"></a>00373 NMSE is 0.142 on Training Set and 0.146 on Test Set
<a name="l00374"></a>00374 NMSE is 0.141 on Training Set and 0.143 on Test Set
<a name="l00375"></a>00375 NMSE is 0.146 on Training Set and 0.141 on Test Set
<a name="l00376"></a>00376 NMSE is 0.144 on Training Set and 0.141 on Test Set
<a name="l00377"></a>00377 NMSE is 0.140 on Training Set and 0.142 on Test Set
<a name="l00378"></a>00378 NMSE is 0.144 on Training Set and 0.148 on Test Set
<a name="l00379"></a>00379 NMSE is 0.140 on Training Set and 0.139 on Test Set - saving Weights ...
<a name="l00380"></a>00380 NMSE is 0.140 on Training Set and 0.140 on Test Set
<a name="l00381"></a>00381 NMSE is 0.141 on Training Set and 0.138 on Test Set - saving Weights ...
<a name="l00382"></a>00382 .
<a name="l00383"></a>00383 .
<a name="l00384"></a>00384 .
<a name="l00385"></a>00385 NMSE is 0.104 on Training Set and 0.154 on Test Set
<a name="l00386"></a>00386 NMSE is 0.102 on Training Set and 0.160 on Test Set
<a name="l00387"></a>00387 NMSE is 0.102 on Training Set and 0.160 on Test Set
<a name="l00388"></a>00388 NMSE is 0.100 on Training Set and 0.157 on Test Set
<a name="l00389"></a>00389 NMSE is 0.105 on Training Set and 0.153 on Test Set
<a name="l00390"></a>00390 NMSE is 0.100 on Training Set and 0.155 on Test Set
<a name="l00391"></a>00391 NMSE is 0.101 on Training Set and 0.154 on Test Set
<a name="l00392"></a>00392 NMSE is 0.100 on Training Set and 0.158 on Test Set
<a name="l00393"></a>00393 NMSE is 0.107 on Training Set and 0.170 on Test Set - stopping Training
<a name="l00394"></a>00394                                                       and restoring Weights ...
<a name="l00395"></a>00395 NMSE is 0.141 on Training Set and 0.138 on Test Set
<a name="l00396"></a>00396 
<a name="l00397"></a>00397 
<a name="l00398"></a>00398 Year    Sunspots    Open-Loop Prediction    Closed-Loop Prediction
<a name="l00399"></a>00399 
<a name="l00400"></a>00400 1960       0.572                   0.532                     0.532
<a name="l00401"></a>00401 1961       0.327                   0.334                     0.301
<a name="l00402"></a>00402 1962       0.258                   0.158                     0.146
<a name="l00403"></a>00403 1963       0.217                   0.156                     0.098
<a name="l00404"></a>00404 1964       0.143                   0.236                     0.149
<a name="l00405"></a>00405 1965       0.164                   0.230                     0.273
<a name="l00406"></a>00406 1966       0.298                   0.263                     0.405
<a name="l00407"></a>00407 1967       0.495                   0.454                     0.552
<a name="l00408"></a>00408 1968       0.545                   0.615                     0.627
<a name="l00409"></a>00409 1969       0.544                   0.550                     0.589
<a name="l00410"></a>00410 1970       0.540                   0.474                     0.464
<a name="l00411"></a>00411 1971       0.380                   0.455                     0.305
<a name="l00412"></a>00412 1972       0.390                   0.270                     0.191
<a name="l00413"></a>00413 1973       0.260                   0.275                     0.139
<a name="l00414"></a>00414 1974       0.245                   0.211                     0.158
<a name="l00415"></a>00415 1975       0.165                   0.181                     0.170
<a name="l00416"></a>00416 1976       0.153                   0.128                     0.175
<a name="l00417"></a>00417 1977       0.215                   0.151                     0.193
<a name="l00418"></a>00418 1978       0.489                   0.316                     0.274
<a name="l00419"></a>00419 1979       0.754                   0.622                     0.373
<a name="l00420"></a>00420 
</pre></div></div>
</div>
<!--- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Data Structures</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Typedefs</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Defines</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<hr class="footer"/><address class="footer"><small>Generated on Fri Oct 19 2018 10:35:44 for Nilorea Library by&#160;
<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.7.2 </small></address>
</body>
</html>
